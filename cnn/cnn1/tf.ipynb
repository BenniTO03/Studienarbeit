{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(images_path):\n",
    "    # speichert Bilder als numpy array\n",
    "\n",
    "    array_images = []\n",
    "    train_or_test_folder = os.listdir(images_path)\n",
    "\n",
    "    for folder in natsorted(train_or_test_folder):\n",
    "        single_folder = os.path.join(images_path, folder)\n",
    "\n",
    "        for file in os.listdir(single_folder):\n",
    "            filepath = os.path.join(single_folder, file)\n",
    "\n",
    "            if filepath.lower().endswith(('.jpeg', '.jpg')):\n",
    "                image = cv2.resize(cv2.imread(filepath), (64, 64))  # resize Größe bestimmt durch vortainiertes Netz\n",
    "                array_images.append(image)\n",
    "\n",
    "    images = np.array(array_images)\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(images_path):\n",
    "    # speichert Lables als numpy array\n",
    "\n",
    "    array_label = []\n",
    "    for folder in natsorted(os.listdir(images_path)):\n",
    "        label = int(folder)\n",
    "\n",
    "        for file in os.listdir(os.path.join(images_path, folder)):\n",
    "            array_label.append(label)\n",
    "    labels = np.array(array_label)\n",
    "        \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = get_images(\"../../02_data_crop/train\") # train images\n",
    "labels = get_label(\"../../02_data_crop/train\")  # train labels\n",
    "\n",
    "X_eval = get_images(\"../../02_data_crop/test\")  # Evaluierungs Bilder\n",
    "y_eval = get_label(\"../../02_data_crop/test\")   # Evaluierungs Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multilineare Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.learn as learn\n",
    "import tf\n",
    "\n",
    "\n",
    "dset = learn.datasets.mnist.read_data_sets('MNIST-data', one_hot=True)\n",
    "print('Trainingsbilder: ',dset.train.images.shape)            # (55000,784)\n",
    "print('Validierungsbilder: ',dset.validation.images.shape)    # (5000, 784)\n",
    "print('Testbilder: ',dset.test.images.shape)                  # (10000, 784)\n",
    "\n",
    "image_holder = tf.placeholder(tf.float32, [None, 784])\n",
    "label_holder = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "m = tf.Variable(tf.zeros([784, 27]))\n",
    "b = tf.Variable(tf.zeros([27]))\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=tf.matmul(image_holder, m) + b, labels= label_holder))\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 25\n",
    "batch_size = 100\n",
    "num_batches = int(dset.train.num_examples/batch_size)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range (num_epochs):\n",
    "        for batch in range(num_batches):\n",
    "            image_batch, label_batch = dset.train.next_batch(batch_size)\n",
    "            _, lossVal = sess.run([optimizer, loss],\n",
    "                                  feed_dict={image_holder: image_batch,\n",
    "                                             label_batch: label_batch})\n",
    "            \n",
    "    print('Finaler Verlust: ', lossVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eingaben standisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = tf.constant([1., 3., 5., 7., 9.])\n",
    "stat_mean, stat_var = tf.nn.moments(input_data, 0)\n",
    "standard_data = tf.nn.batch_normalization(input_data, stat_mean, stat_var, 0., 1., 0.0001, name=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gewichte initialisieren"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
