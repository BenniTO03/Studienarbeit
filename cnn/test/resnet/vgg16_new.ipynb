{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from keras.models import Model \n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.layers import GlobalMaxPool2D\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.preprocessing import image\n",
    "#from keras.utils import layers\n",
    "from keras.utils import get_file\n",
    "from keras import backend as K\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "#from keras.applications.imagenet_utils import obtain_input_shape\n",
    "#from keras.engine.topology import get_source_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGGUpdated(input_tensor=None, classes=26):\n",
    "    img_rows, img_cols = 224, 224 # by default size is 224, 224\n",
    "    img_channels = 3\n",
    "\n",
    "    img_dim = (img_rows, img_cols, img_channels)\n",
    "    img_input = Input(shape=img_dim)\n",
    "\n",
    "    x = Conv2D(64, (3,3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "    x = Conv2D(64, (3,3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "    x = MaxPool2D((2,2), strides=(2,2), name='block1_pool')(x)\n",
    "\n",
    "    x = Conv2D(128, (3,3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "    x = Conv2D(128, (3,3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "    x = MaxPool2D((2,2), strides=(2,2), name='block2_pool')(x)\n",
    "\n",
    "    x = Conv2D(256, (3,3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "    x = Conv2D(256, (3,3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "    x = Conv2D(256, (3,3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "    x = MaxPool2D((2,2), strides=(2,2), name='block3_pool')(x)\n",
    "\n",
    "    x = Conv2D(512, (3,3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "    x = Conv2D(512, (3,3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "    x = Conv2D(512, (3,3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "    x = MaxPool2D((2,2), strides=(2,2), name='block4_pool')(x)\n",
    "\n",
    "    x = Conv2D(512, (3,3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "    x = Conv2D(512, (3,3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "    x = Conv2D(512, (3,3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "    x = MaxPool2D((2,2), strides=(2,2), name='block5_pool')(x)\n",
    "\n",
    "    # classification block\n",
    "    x = Flatten(name='flatten')(x)\n",
    "    x = Dense(4096, activation='relu', name='fc1')(x)\n",
    "    x = Dense(4096, activation='relu', name='fc2')(x)\n",
    "    x = Dense(classes, activation='softmax', name='predictions')(x)\n",
    "\n",
    "    model = Model(inputs=img_input, outputs=x, name='VGG')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGGUpdated(classes=26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types of classes found:  26\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "dataset_path = os.listdir('../../02_data_crop_abc/train')\n",
    "classes = os.listdir('../../02_data_crop_abc/train')\n",
    "print(\"Types of classes found: \", len(dataset_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagePath_label_pair = []\n",
    "for item in classes:\n",
    "    all_letters = os.listdir('../../02_data_crop_abc/train' + '/' + item)\n",
    "    for letter in all_letters:\n",
    "        imagePath_label_pair.append((item, str('../../02_data_crop_abc/train' + '/' + item) + '/' + letter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\nletter_df = pd.DataFrame(data=letters, columns=['letter', 'image'])\\n\\n# Definieren der Batch-Größe\\nbatch_size = 64\\n\\n# Aufteilen des DataFrames in Batches\\nbatches = [letter_df[i:i+batch_size] for i in range(0, len(letter_df), batch_size)]\\n\\n# Iteration über die Batches\\nfor batch in batches:\\n    # Hier können Sie Ihre Verarbeitungsschritte für jeden Batch durchführen\\n    print(batch)\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\" \n",
    "letter_df = pd.DataFrame(data=letters, columns=['letter', 'image'])\n",
    "\n",
    "# Definieren der Batch-Größe\n",
    "batch_size = 64\n",
    "\n",
    "# Aufteilen des DataFrames in Batches\n",
    "batches = [letter_df[i:i+batch_size] for i in range(0, len(letter_df), batch_size)]\n",
    "\n",
    "# Iteration über die Batches\n",
    "for batch in batches:\n",
    "    # Hier können Sie Ihre Verarbeitungsschritte für jeden Batch durchführen\n",
    "    print(batch)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  letter                                       image\n",
      "0      a  ../../02_data_crop_abc/train/a/a_00001.jpg\n",
      "1      a  ../../02_data_crop_abc/train/a/a_00002.jpg\n",
      "2      a  ../../02_data_crop_abc/train/a/a_00003.jpg\n",
      "3      a  ../../02_data_crop_abc/train/a/a_00004.jpg\n",
      "4      a  ../../02_data_crop_abc/train/a/a_00005.jpg\n"
     ]
    }
   ],
   "source": [
    "# build a dataframe\n",
    "letter_imagePath = pd.DataFrame(data=imagePath_label_pair, columns=['letter', 'image'])\n",
    "print(letter_imagePath.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number:  10368\n",
      "images in each class: \n",
      "letter\n",
      "a    494\n",
      "e    449\n",
      "b    444\n",
      "i    440\n",
      "l    430\n",
      "n    428\n",
      "k    426\n",
      "v    425\n",
      "o    417\n",
      "t    409\n",
      "s    408\n",
      "u    407\n",
      "w    402\n",
      "m    398\n",
      "r    398\n",
      "p    396\n",
      "h    395\n",
      "d    393\n",
      "g    392\n",
      "f    390\n",
      "c    389\n",
      "q    386\n",
      "x    381\n",
      "y    373\n",
      "j    321\n",
      "z    177\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number: \", len(letter_imagePath))\n",
    "letter_count = letter_imagePath['letter'].value_counts()\n",
    "print('images in each class: ')\n",
    "print(letter_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "import cv2\n",
    "path = '../../02_data_crop_abc/train/'\n",
    "\n",
    "img_size = 224\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "batch_size = 64\n",
    "for i in class_types:\n",
    "    data_path = path + str(i)\n",
    "    filenames = [i for i in os.listdir(data_path)]\n",
    "    for j in range(0, len(filenames), batch_size):\n",
    "        batch_images = []\n",
    "        batch_labels = []\n",
    "        for f in filenames[j:j+batch_size]:\n",
    "            img = cv2.imread(data_path + '/' + f)\n",
    "            img = cv2.resize(img, (img_size, img_size))\n",
    "            batch_images.append(img)\n",
    "            batch_labels.append(i)\n",
    "        batch_images = np.array(batch_images)\n",
    "        batch_images = batch_images.astype('float32') / 255.0\n",
    "        images.append(batch_images)\n",
    "        labels.extend(batch_labels)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "path = '../../02_data_crop_abc/train/'\n",
    "\n",
    "img_size = 224\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "for i in classes:\n",
    "    data_path = path + str(i)\n",
    "    filenames = [i for i in os.listdir(data_path)]\n",
    "    for f in filenames:\n",
    "        img = cv2.imread(data_path + '/' + f)\n",
    "        img = cv2.resize(img, (img_size, img_size))\n",
    "        images.append(img)\n",
    "        labels.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10368, 224, 224, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "images = np.array(images)\n",
    "images = images.astype('float32') / 255.0\n",
    "images.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0 ... 25 25 25]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "y = letter_imagePath['letter'].values\n",
    "\n",
    "y_labelencoder = LabelEncoder()\n",
    "y = y_labelencoder.fit_transform(y)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10368, 26)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y.reshape(-1,1)\n",
    "onehotencoder = OneHotEncoder()\n",
    "Y = onehotencoder.fit_transform(y)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9849, 224, 224, 3)\n",
      "(9849, 26)\n",
      "(519, 224, 224, 3)\n",
      "(519, 26)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "images, Y = shuffle(images, Y, random_state=1)\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(images, Y, test_size=0.05, random_state=415)\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Failed to convert elements of SparseTensor(indices=Tensor(\"data_1:0\", shape=(None, 2), dtype=int64), values=Tensor(\"compile_loss/categorical_crossentropy/Cast:0\", shape=(None,), dtype=float32), dense_shape=Tensor(\"data_3:0\", shape=(2,), dtype=int64)) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m train_generator \u001b[38;5;241m=\u001b[39m data_generator(train_x, train_y, batch_size)\n\u001b[0;32m     11\u001b[0m test_generator \u001b[38;5;241m=\u001b[39m data_generator(test_x, test_y, batch_size)\n\u001b[1;32m---> 13\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KimJu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\KimJu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py:562\u001b[0m, in \u001b[0;36mcategorical_crossentropy\u001b[1;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcategorical_crossentropy\u001b[39m(target, output, from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    524\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Categorical crossentropy between an output tensor and a target tensor.\u001b[39;00m\n\u001b[0;32m    525\u001b[0m \n\u001b[0;32m    526\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;124;03m    [0. 0. 0.]\u001b[39;00m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 562\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    563\u001b[0m     output \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(output)\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: Failed to convert elements of SparseTensor(indices=Tensor(\"data_1:0\", shape=(None, 2), dtype=int64), values=Tensor(\"compile_loss/categorical_crossentropy/Cast:0\", shape=(None,), dtype=float32), dense_shape=Tensor(\"data_3:0\", shape=(2,), dtype=int64)) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes."
     ]
    }
   ],
   "source": [
    "def data_generator(images, labels, batch_size):\n",
    "    num_samples = len(images)\n",
    "    while True:\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch_images = images[i:i+batch_size]\n",
    "            batch_labels = labels[i:i+batch_size]\n",
    "            yield batch_images, batch_labels\n",
    "\n",
    "batch_size = 64\n",
    "train_generator = data_generator(train_x, train_y, batch_size)\n",
    "test_generator = data_generator(test_x, test_y, batch_size)\n",
    "\n",
    "model.fit(train_generator, epochs=5, steps_per_epoch=len(train_x)//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 5.52 GiB for an array with shape (9849, 224, 224, 3) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KimJu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\KimJu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:96\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Converts the given `value` to an `EagerTensor`.\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03mNote that this function could return cached copies of created constants for\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m  TypeError: if `dtype` is not compatible with the type of t.\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m     93\u001b[0m   \u001b[38;5;66;03m# Make a copy explicitly because the EagerTensor might share the underlying\u001b[39;00m\n\u001b[0;32m     94\u001b[0m   \u001b[38;5;66;03m# memory with the input array. Without this copy, users will be able to\u001b[39;00m\n\u001b[0;32m     95\u001b[0m   \u001b[38;5;66;03m# modify the EagerTensor after its creation by changing the input array.\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m   value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, ops\u001b[38;5;241m.\u001b[39mEagerTensor):\n\u001b[0;32m     98\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m dtype:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 5.52 GiB for an array with shape (9849, 224, 224, 3) and data type float32"
     ]
    }
   ],
   "source": [
    "#model.fit(train_x, train_y, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.evaluate(test_x, test_y)\n",
    "print(\"Loss= \" + str(preds[0]))\n",
    "print(\"Test acc= \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_path = 'test.jpg'\n",
    "img = image.load_img(img_path, target_size=(224,224))\n",
    "\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "print('Input image shape: ', x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_image = plt.imread(img_path)\n",
    "plt.imshow(my_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
